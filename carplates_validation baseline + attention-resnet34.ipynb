{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "#for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#    for filename in filenames:\n",
    "#        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train', 'train.json', 'test', 'submission.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('../input/made-cv-hw2/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import json\n",
    "import glob\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.path import Path\n",
    "import seaborn as sns\n",
    "import tqdm\n",
    "\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "from torchvision import transforms\n",
    "\n",
    "import PIL\n",
    "from PIL import Image, ImageDraw\n",
    "import cv2\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Бинарный поиск для приближения предсказанной маски 4-хугольником\n",
    "def simplify_contour(contour, n_corners=4):\n",
    "    n_iter, max_iter = 0, 1000\n",
    "    lb, ub = 0., 1.\n",
    "\n",
    "    while True:\n",
    "        n_iter += 1\n",
    "        if n_iter > max_iter:\n",
    "            print('simplify_contour didnt coverege')\n",
    "            return None\n",
    "\n",
    "        k = (lb + ub)/2.\n",
    "        eps = k*cv2.arcLength(contour, True)\n",
    "        approx = cv2.approxPolyDP(contour, eps, True)\n",
    "\n",
    "        if len(approx) > n_corners:\n",
    "            lb = (lb + ub)/2.\n",
    "        elif len(approx) < n_corners:\n",
    "            ub = (lb + ub)/2.\n",
    "        else:\n",
    "            return approx\n",
    "\n",
    "# Отображаем 4-хугольник в прямоугольник \n",
    "# Спасибо ulebok за идею \n",
    "# И вот этим ребятам за реализацию: https://www.pyimagesearch.com/2014/08/25/4-point-opencv-getperspective-transform-example/\n",
    "def four_point_transform(image, pts):\n",
    "    \n",
    "    rect = order_points(pts)\n",
    "    \n",
    "    tl, tr, br, bl = pts\n",
    "    \n",
    "    width_1 = np.sqrt(((br[0] - bl[0]) ** 2) + ((br[1] - bl[1]) ** 2))\n",
    "    width_2 = np.sqrt(((tr[0] - tl[0]) ** 2) + ((tr[1] - tl[1]) ** 2))\n",
    "    max_width = max(int(width_1), int(width_2))\n",
    "    \n",
    "    height_1 = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[1]) ** 2))\n",
    "    height_2 = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tl[1] - bl[1]) ** 2))\n",
    "    max_height = max(int(height_1), int(height_2))\n",
    "    \n",
    "    dst = np.array([\n",
    "        [0, 0],\n",
    "        [max_width, 0],\n",
    "        [max_width, max_height],\n",
    "        [0, max_height]], dtype = \"float32\")\n",
    "    \n",
    "    M = cv2.getPerspectiveTransform(rect, dst)\n",
    "    warped = cv2.warpPerspective(image, M, (max_width, max_height))\n",
    "    return warped\n",
    "\n",
    "def order_points(pts):\n",
    "    rect = np.zeros((4, 2), dtype = \"float32\")\n",
    "    \n",
    "    s = pts.sum(axis = 1)\n",
    "    rect[0] = pts[np.argmin(s)]\n",
    "    rect[2] = pts[np.argmax(s)]\n",
    "    \n",
    "    diff = np.diff(pts, axis = 1)\n",
    "    rect[1] = pts[np.argmin(diff)]\n",
    "    rect[3] = pts[np.argmax(diff)]\n",
    "    \n",
    "    return rect\n",
    "\n",
    "\n",
    "# Визуализируем детекцию (4 точки, bounding box и приближенный по маске контур)\n",
    "def visualize_prediction_plate(file, model, device='cuda', verbose=True, thresh=0.0, \n",
    "                               n_colors=None, id_to_name=None):\n",
    "    img = Image.open(file)\n",
    "    img_tensor = my_transforms(img)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model([img_tensor.to(device)])\n",
    "    prediction = predictions[0]\n",
    "    \n",
    "    if n_colors is None:\n",
    "        n_colors = model.roi_heads.box_predictor.cls_score.out_features\n",
    "    \n",
    "    palette = sns.color_palette(None, n_colors)\n",
    "    \n",
    "    img = cv2.imread(file, cv2.COLOR_BGR2RGB)\n",
    "    h, w = img.shape[:2]\n",
    "    image = img\n",
    "    \n",
    "    blackImg = np.zeros(image.shape, image.dtype)\n",
    "    blackImg[:,:] = (0, 0, 0)\n",
    "    for i in range(len(prediction['boxes'])):\n",
    "        x_min, y_min, x_max, y_max = map(int, prediction['boxes'][i].tolist())\n",
    "        label = int(prediction['labels'][i].cpu())\n",
    "        score = float(prediction['scores'][i].cpu())\n",
    "        mask = prediction['masks'][i][0, :, :].cpu().numpy()\n",
    "        name = id_to_name[label]\n",
    "        color = palette[label]\n",
    "        \n",
    "        if verbose:\n",
    "            if score > thresh:\n",
    "                print ('Class: {}, Confidence: {}'.format(name, score))\n",
    "        if score > thresh:            \n",
    "            crop_img = image[y_min:y_max, x_min:x_max]\n",
    "            print('Bounding box:')\n",
    "            show_image(crop_img, figsize=(10, 2))\n",
    "            \n",
    "            # В разных версиях opencv этот метод возвращает разное число параметров\n",
    "            # Оставил для версии colab\n",
    "#             _,contours,_ = cv2.findContours((mask > 0.05).astype(np.uint8), 1, 1)\n",
    "            contours,_ = cv2.findContours((mask > 0.05).astype(np.uint8), 1, 1)\n",
    "            approx = simplify_contour(contours[0], n_corners=4)\n",
    "            \n",
    "            if approx is None:\n",
    "                x0, y0 = x_min, y_min\n",
    "                x1, y1 = x_max, y_min\n",
    "                x2, y2 = x_min, y_max\n",
    "                x3, y3 = x_max, y_max\n",
    "#                 points = [[x_min, y_min], [x_min, y_max], [x_max, y_min],[x_max, y_max]]\n",
    "            else:\n",
    "                x0, y0 = approx[0][0][0], approx[0][0][1]\n",
    "                x1, y1 = approx[1][0][0], approx[1][0][1]\n",
    "                x2, y2 = approx[2][0][0], approx[2][0][1]\n",
    "                x3, y3 = approx[3][0][0], approx[3][0][1]\n",
    "                \n",
    "            points = [[x0, y0], [x2, y2], [x1, y1],[x3, y3]]\n",
    "            \n",
    "            \n",
    "            points = np.array(points)\n",
    "            crop_mask_img = four_point_transform(img, points)\n",
    "            print('Rotated img:')\n",
    "            crop_mask_img = cv2.resize(crop_mask_img, (320, 64), interpolation=cv2.INTER_AREA)\n",
    "            show_image(crop_mask_img, figsize=(10, 2))\n",
    "            if approx is not None:\n",
    "                cv2.drawContours(image, [approx], 0, (255,0,255), 3)\n",
    "            image = cv2.circle(image, (x0, y0), radius=5, color=(0, 0, 255), thickness=-1)\n",
    "            image = cv2.circle(image, (x1, y1), radius=5, color=(0, 0, 255), thickness=-1)\n",
    "            image = cv2.circle(image, (x2, y2), radius=5, color=(0, 0, 255), thickness=-1)\n",
    "            image = cv2.circle(image, (x3, y3), radius=5, color=(0, 0, 255), thickness=-1)\n",
    "            \n",
    "            image = cv2.rectangle(image, (x_min, y_min), (x_max, y_max), np.array(color) * 255, 2)\n",
    "            \n",
    "    show_image(image)\n",
    "    return prediction\n",
    "\n",
    "# Просто показать картинку. С семинара\n",
    "def show_image(image, figsize=(16, 9), reverse=True):\n",
    "    plt.figure(figsize=figsize)\n",
    "    if reverse:\n",
    "        plt.imshow(image[...,::-1])\n",
    "    else:\n",
    "        plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "# Переводит предсказания модели в текст. С семинара\n",
    "def decode(pred, alphabet):\n",
    "    pred = pred.permute(1, 0, 2).cpu().data.numpy()\n",
    "    outputs = []\n",
    "    for i in range(len(pred)):\n",
    "        outputs.append(pred_to_string(pred[i], alphabet))\n",
    "    return outputs\n",
    "\n",
    "def pred_to_string(pred, alphabet):\n",
    "    seq = []\n",
    "    for i in range(len(pred)):\n",
    "        label = np.argmax(pred[i])\n",
    "        seq.append(label - 1)\n",
    "    out = []\n",
    "    for i in range(len(seq)):\n",
    "        if len(out) == 0:\n",
    "            if seq[i] != -1:\n",
    "                out.append(seq[i])\n",
    "        else:\n",
    "            if seq[i] != -1 and seq[i] != seq[i - 1]:\n",
    "                out.append(seq[i])\n",
    "    out = ''.join([alphabet[c] for c in out])\n",
    "    return out\n",
    "        \n",
    "def load_json(file):\n",
    "    with open(file, 'r') as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "# Чтобы без проблем сериализовывать json. Без него есть нюансы\n",
    "class npEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.int32):\n",
    "            return int(obj)\n",
    "        return json.JSONEncoder.default(self, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../input/made-cv-hw2/data/'\n",
    "TRAIN_SIZE = 0.9\n",
    "BATCH_SIZE = 2\n",
    "BATCH_SIZE_OCR = 128#16\n",
    "DETECTOR_MODEL_PATH = '../input/detector4carplates/detector.pt'\n",
    "OCR_MODEL_PATH = 'ocr.pt'\n",
    "\n",
    "all_marks = load_json(os.path.join(DATA_PATH, 'train.json'))\n",
    "test_start = int(TRAIN_SIZE * len(all_marks))\n",
    "train_marks = all_marks[:test_start]\n",
    "val_marks = all_marks[test_start:]\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_detector_model():\n",
    "    \n",
    "    model = models.detection.maskrcnn_resnet50_fpn(\n",
    "        pretrained=True, \n",
    "        pretrained_backbone=True,\n",
    "        progress=True, \n",
    "        num_classes=91, \n",
    "    )\n",
    "\n",
    "    num_classes = 2\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    \n",
    "    box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    model.roi_heads.box_predictor = box_predictor\n",
    "    \n",
    "    mask_predictor = MaskRCNNPredictor(256, 256, num_classes)\n",
    "    model.roi_heads.mask_predictor = mask_predictor\n",
    "\n",
    "    # Заморозим все слои кроме последних\n",
    "    \n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    for param in model.backbone.fpn.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    for param in model.rpn.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    for param in model.roi_heads.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\" to /root/.cache/torch/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adf23f53eeba452cb07f1da656da31fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=178090079.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "model = get_detector_model()\n",
    "model.load_state_dict(torch.load(DETECTOR_MODEL_PATH))#, map_location=torch.device('cpu')))\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = glob.glob(os.path.join(DATA_PATH, 'test/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_transforms = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3188 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/opt/conda/conda-bld/pytorch_1587428398394/work/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero(Tensor input, *, Tensor out)\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(Tensor input, *, bool as_tuple)\n",
      " 85%|████████▌ | 2725/3188 [08:17<01:19,  5.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simplify_contour didnt coverege\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    }
   ],
   "source": [
    "THRESHOLD_SCORE = 0.93\n",
    "TRESHOLD_MASK = 0.05\n",
    "\n",
    "preds = []\n",
    "model.eval()\n",
    "\n",
    "\n",
    "for file in tqdm.tqdm(test_images, position=0, leave=False):\n",
    "\n",
    "    img = Image.open(file).convert('RGB')\n",
    "    img_tensor = my_transforms(img)\n",
    "    with torch.no_grad():\n",
    "        predictions = model([img_tensor.to(device)])\n",
    "    prediction = predictions[0]\n",
    "\n",
    "    pred = dict()\n",
    "    pred['file'] = file\n",
    "    pred['nums'] = []\n",
    "\n",
    "    for i in range(len(prediction['boxes'])):\n",
    "        x_min, y_min, x_max, y_max = map(int, prediction['boxes'][i].tolist())\n",
    "        label = int(prediction['labels'][i].cpu())\n",
    "        score = float(prediction['scores'][i].cpu())\n",
    "        mask = prediction['masks'][i][0, :, :].cpu().numpy()\n",
    "\n",
    "        if score > THRESHOLD_SCORE:      \n",
    "            # В разных версиях opencv этот метод возвращает разное число параметров\n",
    "            # Оставил для версии colab\n",
    "            contours,_ = cv2.findContours((mask > TRESHOLD_MASK).astype(np.uint8), 1, 1)\n",
    "#             _,contours,_ = cv2.findContours((mask > TRESHOLD_MASK).astype(np.uint8), 1, 1)\n",
    "            approx = simplify_contour(contours[0], n_corners=4)\n",
    "            \n",
    "            if approx is None:\n",
    "                x0, y0 = x_min, y_min\n",
    "                x1, y1 = x_max, y_min\n",
    "                x2, y2 = x_min, y_max\n",
    "                x3, y3 = x_max, y_max\n",
    "            else:\n",
    "                x0, y0 = approx[0][0][0], approx[0][0][1]\n",
    "                x1, y1 = approx[1][0][0], approx[1][0][1]\n",
    "                x2, y2 = approx[2][0][0], approx[2][0][1]\n",
    "                x3, y3 = approx[3][0][0], approx[3][0][1]\n",
    "                \n",
    "            points = [[x0, y0], [x2, y2], [x1, y1],[x3, y3]]\n",
    "\n",
    "            pred['nums'].append({\n",
    "                'box': points,\n",
    "                'bbox': [x_min, y_min, x_max, y_max],\n",
    "            })\n",
    "\n",
    "    preds.append(pred)   \n",
    "\n",
    "DATA_PATH1 = '../working/'\n",
    "with open(os.path.join(DATA_PATH1, 'test.json'), 'w') as json_file:\n",
    "    json.dump(preds, json_file, cls=npEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#abc = \"0123456789ABEKMHOPCTYX\"  # this is our alphabet for predictions.\n",
    "\n",
    "class OCRDataset(Dataset):\n",
    "    def __init__(self, marks, img_folder, alphabet, transforms=None):\n",
    "        ocr_marks = []\n",
    "        for items in marks:\n",
    "            file_path = items['file']\n",
    "            if file_path == 'train/25632.bmp':\n",
    "                continue\n",
    "            for box in items['nums']:\n",
    "                \n",
    "                ocr_marks.append({\n",
    "                    'file': file_path,\n",
    "                    'box': np.clip(box['box'], 0, None).tolist(),\n",
    "                    'text': box['text'],\n",
    "                    'boxed': False,\n",
    "                })\n",
    "                            \n",
    "                # Добавим точки, запакованные в BoundingBox. \n",
    "                # Вместо аугментации rotate. Датасет будет в 2 раза больше\n",
    "                \n",
    "                #Клипаем, ибо есть отрицательные координаты\n",
    "                points = np.clip(box['box'], 0, None) \n",
    "                x0, y0 = np.min(points[:, 0]), np.min(points[:, 1])\n",
    "                x2, y2 = np.max(points[:, 0]), np.max(points[:, 1])\n",
    "\n",
    "                ocr_marks.append({\n",
    "                    'file': file_path,\n",
    "                    'box': [x0, y0, x2, y2],\n",
    "                    'text': box['text'],\n",
    "                    'boxed': True,\n",
    "                })\n",
    "                \n",
    "        self.marks = ocr_marks\n",
    "        self.img_folder = img_folder\n",
    "        self.transforms = transforms\n",
    "        self.alphabet = alphabet\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.marks[idx]\n",
    "        img_path = os.path.join(self.img_folder, item[\"file\"])\n",
    "        img = cv2.imread(img_path)\n",
    "\n",
    "        if item['boxed']:\n",
    "            x_min, y_min, x_max, y_max = item['box']\n",
    "            img = img[y_min:y_max, x_min:x_max]\n",
    "        else:\n",
    "            points = np.clip(np.array(item['box']), 0, None)\n",
    "            img = four_point_transform(img, points)\n",
    "            \n",
    "        text = item['text']\n",
    "        seq = [self.alphabet.find(char) + 1 for char in text]\n",
    "        seq_len = len(seq)\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        output = {\n",
    "            'img': img,\n",
    "            'text': text,\n",
    "            'seq': seq,\n",
    "            'seq_len': seq_len\n",
    "        }\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.marks)\n",
    "    \n",
    "    \n",
    "class Resize(object):\n",
    "    def __init__(self, size=(320, 64)):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, img):\n",
    "\n",
    "        w_from, h_from = img.shape[1], img.shape[0]\n",
    "        w_to, h_to = self.size\n",
    "        \n",
    "        # Сделаем разную интерполяцию при увеличении и уменьшении\n",
    "        # Если увеличиваем картинку, меняем интерполяцию\n",
    "        interpolation = cv2.INTER_AREA\n",
    "        if w_to > w_from:\n",
    "            interpolation = cv2.INTER_CUBIC\n",
    "        \n",
    "        img = cv2.resize(img, dsize=self.size, interpolation=interpolation)\n",
    "        return img\n",
    "    \n",
    "my_ocr_transforms = transforms.Compose([\n",
    "    Resize(size=(320, 64)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "def get_vocab_from_marks(marks):\n",
    "    train_texts = []\n",
    "    for item in marks:\n",
    "        for num in item['nums']:\n",
    "            train_texts.append(num['text'])\n",
    "\n",
    "    counts = Counter(''.join(train_texts))\n",
    "    alphabet = ''.join(set(''.join(train_texts)))\n",
    "    corted_counts = sorted(counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    char_to_idx = {item[0]: idx + 1 for idx, item in enumerate(corted_counts)}\n",
    "    idx_to_char = {idx:char for char, idx in char_to_idx.items()}\n",
    "    return char_to_idx, idx_to_char, alphabet\n",
    "\n",
    "char_to_idx, idx_to_char, alphabet = get_vocab_from_marks(train_marks)\n",
    "\n",
    "train_ocr_dataset = OCRDataset(\n",
    "    marks=train_marks, \n",
    "    img_folder=DATA_PATH, \n",
    "    alphabet=alphabet,\n",
    "    transforms=my_ocr_transforms\n",
    ")\n",
    "val_ocr_dataset = OCRDataset(\n",
    "    marks=val_marks, \n",
    "    img_folder=DATA_PATH, \n",
    "    alphabet=alphabet,\n",
    "    transforms=my_ocr_transforms\n",
    ")\n",
    "\n",
    "def collate_fn_ocr(batch):\n",
    "    \"\"\"Function for torch.utils.data.Dataloader for batch collecting.\n",
    "    Accepts list of dataset __get_item__ return values (dicts).\n",
    "    Returns dict with same keys but values are either torch.Tensors of batched images, sequences, and so.\n",
    "    \"\"\"\n",
    "    images, seqs, seq_lens, texts = [], [], [], []\n",
    "    for sample in batch:\n",
    "        images.append(sample[\"img\"])\n",
    "        seqs.extend(sample[\"seq\"])\n",
    "        seq_lens.append(sample[\"seq_len\"])\n",
    "        texts.append(sample[\"text\"])\n",
    "    images = torch.stack(images)\n",
    "    seqs = torch.Tensor(seqs).int()\n",
    "    seq_lens = torch.Tensor(seq_lens).int()\n",
    "    batch = {\"image\": images, \"seq\": seqs, \"seq_len\": seq_lens, \"text\": texts}\n",
    "    return batch\n",
    "\n",
    "train_ocr_loader = DataLoader(\n",
    "    train_ocr_dataset, \n",
    "    batch_size=BATCH_SIZE_OCR, \n",
    "    drop_last=True,\n",
    "    num_workers=0, # Почему-то у меня виснет DataLoader, если запустить несколько потоков\n",
    "    collate_fn=collate_fn_ocr,\n",
    "    timeout=0,\n",
    "    shuffle=True # Чтобы повернутые дубли картинок не шли подряд\n",
    ")\n",
    "\n",
    "val_ocr_loader = DataLoader(\n",
    "    val_ocr_dataset, \n",
    "    batch_size=BATCH_SIZE_OCR, \n",
    "    drop_last=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn_ocr, \n",
    "    timeout=0,\n",
    ")\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = \"0123456789ABCEHKMOPTXY\"\n",
    "\n",
    "mapping = {\n",
    "    'А': 'A',\n",
    "    'В': 'B',\n",
    "    'С': 'C',\n",
    "    'Е': 'E',\n",
    "    'Н': 'H',\n",
    "    'К': 'K',\n",
    "    'М': 'M',\n",
    "    'О': 'O',\n",
    "    'Р': 'P',\n",
    "    'Т': 'T',\n",
    "    'Х': 'X',\n",
    "    'У': 'Y',\n",
    "}\n",
    "\n",
    "def labels_to_text(labels, abc=abc):\n",
    "    return ''.join(list(map(lambda x: abc[int(x) - 1], labels)))\n",
    "\n",
    "def text_to_labels(text, abc=abc):\n",
    "    return list(map(lambda x: abc.index(x) + 1, text))\n",
    "\n",
    "def is_valid_str(s, abc=abc):\n",
    "    for ch in s:\n",
    "        if ch not in abc:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def convert_to_eng(text, mapping=mapping):\n",
    "    return ''.join([mapping.get(a, a) for a in text])\n",
    "\n",
    "def return_zero_to_num(x):\n",
    "    return x[0] + x[1:4].replace('O', '0') + x[4:6] + x[6:].replace('O', '0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try attention model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class strLabelConverterForAttention(object):\n",
    "    def __init__(self, alphabet):\n",
    "        self.alphabet = alphabet\n",
    "\n",
    "        self.dict = {}\n",
    "        self.dict['SOS'] = 0\n",
    "        self.dict['EOS'] = 1\n",
    "        self.dict['$'] = 2\n",
    "        for i, item in enumerate(self.alphabet):\n",
    "            self.dict[item] = i + 3\n",
    "\n",
    "    def encode(self, text):\n",
    "        if isinstance(text, str):\n",
    "            text = [self.dict[item] for item in text]\n",
    "        elif isinstance(text, list):\n",
    "            text = [self.encode(s) for s in text]\n",
    "\n",
    "            max_length = max([len(x) for x in text])\n",
    "            nb = len(text)\n",
    "            targets = torch.ones(nb, max_length + 2) * 2\n",
    "            for i in range(nb):\n",
    "                targets[i][0] = 0\n",
    "                targets[i][1:len(text[i]) + 1] = text[i]\n",
    "                targets[i][len(text[i]) + 1] = 1\n",
    "            text = targets.transpose(0, 1).contiguous()\n",
    "            text = text.long()\n",
    "        return torch.LongTensor(text)\n",
    "\n",
    "    def decode(self, t):\n",
    "        texts = list(self.dict.keys())[list(self.dict.values()).index(t)]\n",
    "        return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = strLabelConverterForAttention(abc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attentiondecoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
    "        super(Attentiondecoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "        self.vat = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input)\n",
    "        embedded = self.dropout(embedded)\n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        alpha = hidden + encoder_outputs\n",
    "        alpha = alpha.reshape((-1, alpha.shape[-1]))#alpha.view(-1, alpha.shape[-1])\n",
    "        attn_weights = self.vat( torch.tanh(alpha))\n",
    "        attn_weights = attn_weights.view(-1, 1, batch_size).permute((2,1,0))\n",
    "        attn_weights = F.softmax(attn_weights, dim=2)\n",
    "        attn_applied = torch.matmul(attn_weights,\n",
    "                                 encoder_outputs.permute((1, 0, 2)))\n",
    "        output = torch.cat((embedded, attn_applied.squeeze(1) ), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        result = Variable(torch.zeros(1, batch_size, self.hidden_size))\n",
    "        return result\n",
    "\n",
    "\n",
    "class decoder(nn.Module):\n",
    "    def __init__(self, nh, nclass, dropout_p=0.3):\n",
    "        super(decoder, self).__init__()\n",
    "        self.hidden_size = nh\n",
    "        self.decoder = Attentiondecoder(nh, nclass, dropout_p)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        return self.decoder(input, hidden, encoder_outputs)\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        result = Variable(torch.zeros(1, batch_size, self.hidden_size))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_size=(64, 320), hidden_size = 256, num_layers = 2, dropout = 0.3, bidirectional = True):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        h, w = input_size\n",
    "        resnet = getattr(models, 'resnet34')(pretrained=True)\n",
    "        self.cnn = nn.Sequential(*list(resnet.children())[:-3])\n",
    "        self.pool = nn.Conv2d(20, 40, kernel_size=(4,1)).to(device)\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.GRU(input_size=256,\n",
    "                          hidden_size=hidden_size,\n",
    "                          num_layers=num_layers,\n",
    "                          dropout=dropout,\n",
    "                          bidirectional=bidirectional,\n",
    "                          batch_first=True)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        num_directions = 2 if self.rnn.bidirectional else 1\n",
    "        return torch.zeros(self.num_layers * num_directions, batch_size, self.hidden_size)\n",
    "   \n",
    "    def forward(self, x):\n",
    "        features = self.cnn(x)\n",
    "        features = self.pool(features.permute(0, 3, 2, 1).contiguous()).squeeze(2)\n",
    "        batch_size = features.size(0)\n",
    "        h_0 = self.init_hidden(batch_size)\n",
    "        h_0 = h_0.to(x.device)\n",
    "        outputs, hidden = self.rnn(features, h_0)\n",
    "        outputs = (outputs[:, :, :self.hidden_size] + outputs[:, :, self.hidden_size:])\n",
    "        return outputs.permute(1, 0, 2), hidden.sum(0).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "enc = CNN(hidden_size = 128).to(device)\n",
    "dec = decoder(nh = 128, nclass = len(abc) + 3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_optimizer = torch.optim.Adam(enc.parameters(), lr=3e-4, amsgrad=True, weight_decay=1e-5)\n",
    "decoder_optimizer = torch.optim.Adam(dec.parameters(), lr=3e-4, amsgrad=True, weight_decay=1e-5)\n",
    "\n",
    "enc_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(encoder_optimizer, patience=10, factor=0.1, verbose=True)\n",
    "dec_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(decoder_optimizer, patience=10, factor=0.1, verbose=True)\n",
    "criterion = torch.nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "592c94a7e370476ca3eadf34fd1bd9db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=377.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:45: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b3d35396a834833ba08f786860bbc44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=42.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78c8eb602f72457da8dc8031de0f8302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=377.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "169c3e3f6e06448e89d2c6d0bcb5a4da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=42.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cad3d80d99b4350af85a1185a5e7107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=377.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c8c44529b2e4806a5617ae5adf36b9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=42.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ab23b67277d472d90fd6c1d97c2e90f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=377.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a727935069114eaeb5c34ea0a4ffab34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=42.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dc3b0398e084838a6b9f42aebfb7dbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=377.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   179: reducing learning rate of group 0 to 3.0000e-05.\n",
      "Epoch   179: reducing learning rate of group 0 to 3.0000e-05.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "654c484fccfd4e15bd7873fe2eb62e07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=42.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    epoch_losses = []\n",
    "    print_loss = []\n",
    "    with tqdm_notebook(total=len(train_ocr_loader)) as progress_bar:\n",
    "      for i, batch in enumerate(train_ocr_loader):\n",
    "        batch_txt = np.array([convert_to_eng(j) for j in batch['text']])\n",
    "        is_valid_bool = np.array([is_valid_str(i) for i in batch_txt])\n",
    "        src = batch[\"image\"][is_valid_bool].to(device)\n",
    "        trg = converter.encode(list(batch_txt[is_valid_bool])).to(device)\n",
    "        batch_size = trg.shape[1]\n",
    "        max_len = trg.shape[0]\n",
    "        outputs = torch.zeros(max_len, batch_size, len(abc) + 3).to(device)\n",
    "        \n",
    "        conv, hidden = enc(src)\n",
    "        input = trg[0].to(device)\n",
    "        for t in range(1, max_len):\n",
    "            output, hidden, attn_weights = dec(input, hidden, conv)\n",
    "            outputs[t] = output\n",
    "            input = trg[t]\n",
    "        \n",
    "        loss = criterion(outputs[1:, :, :].permute(0, 2, 1), trg[1:, :])\n",
    "\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        print_loss.append(loss.item())\n",
    "        progress_bar.update()\n",
    "        if (i + 1) % 10 == 0:\n",
    "            mean_loss = np.mean(print_loss)\n",
    "            progress_bar.set_description('{:>5s} Loss = {:.4f}'.format(\n",
    "                    'train', mean_loss))\n",
    "            enc_scheduler.step(mean_loss)\n",
    "            dec_scheduler.step(mean_loss)\n",
    "            print_loss = [] \n",
    "    \n",
    "        epoch_losses.append(loss.item())\n",
    "      progress_bar.set_description('{:>5s} Loss = {:.4f}'.format('train_'+str(epoch+1)+'_ep', np.mean(epoch_losses)))\n",
    "\n",
    "    epoch_losses = []\n",
    "    print_loss = []\n",
    "    with torch.no_grad():\n",
    "      with tqdm_notebook(total=len(val_ocr_loader)) as progress_bar:\n",
    "        for i, batch in enumerate(val_ocr_loader):\n",
    "            batch_txt = np.array([convert_to_eng(j) for j in batch['text']])\n",
    "            is_valid_bool = np.array([is_valid_str(i) for i in batch_txt])\n",
    "            src = batch[\"image\"][is_valid_bool].to(device)\n",
    "            trg = converter.encode(list(batch_txt[is_valid_bool])).to(device)\n",
    "            batch_size = trg.shape[1]\n",
    "            max_len = trg.shape[0]\n",
    "            outputs = torch.zeros(max_len, batch_size, len(abc) + 3).to(device)\n",
    "        \n",
    "            conv, hidden = enc(src)\n",
    "            input = trg[0].to(device)\n",
    "            for t in range(1, max_len):\n",
    "                output, hidden, attn_weights = dec(input, hidden, conv)\n",
    "                outputs[t] = output\n",
    "                input = trg[t]\n",
    "        \n",
    "            loss = criterion(outputs[1:, :, :].permute(0, 2, 1), trg[1:, :])\n",
    "            \n",
    "            print_loss.append(loss.item())\n",
    "            progress_bar.update()\n",
    "            if (i + 1) % 10 == 0:\n",
    "                mean_loss = np.mean(print_loss)\n",
    "                progress_bar.set_description('{:>5s} Loss = {:.4f}'.format(\n",
    "                    'val', mean_loss))\n",
    "                print_loss = []\n",
    "    \n",
    "            epoch_losses.append(loss.item())\n",
    "        progress_bar.set_description('{:>5s} Loss = {:.4f}'.format('val_'+str(epoch+1)+'_ep', np.mean(epoch_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_from_arr(arr):\n",
    "  ans = [converter.decode(i) for i in arr]\n",
    "  end_idx = np.argmax(np.array(ans) == 'EOS')\n",
    "  return ''.join(ans[1:end_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 460/3188 [00:18<01:52, 24.30it/s]"
     ]
    }
   ],
   "source": [
    "test_marks = load_json(os.path.join(DATA_PATH1, 'test.json'))\n",
    "enc.eval()\n",
    "dec.eval()\n",
    "resizer = Resize()\n",
    "file_name_result = [] \n",
    "plates_string_result = []\n",
    "batch_size = 2\n",
    "\n",
    "for item in tqdm.tqdm(test_marks, leave=False, position=0):\n",
    "\n",
    "    img_path = item[\"file\"]\n",
    "    img = cv2.imread(img_path)\n",
    "\n",
    "    results_to_sort = []\n",
    "    for box in item['nums']:\n",
    "        x_min, y_min, x_max, y_max = box['bbox']\n",
    "        img_bbox = resizer(img[y_min:y_max, x_min:x_max])\n",
    "        img_bbox = my_transforms(img_bbox)\n",
    "        img_bbox = img_bbox.unsqueeze(0)\n",
    "\n",
    "        points = np.clip(np.array(box['box']), 0, None)\n",
    "        img_polygon = resizer(four_point_transform(img, points))\n",
    "        img_polygon = my_transforms(img_polygon)\n",
    "        img_polygon = img_polygon.unsqueeze(0)\n",
    "\n",
    "        src = torch.cat((img_bbox, img_polygon), dim = 0).to(device)\n",
    "        outputs = torch.zeros(max_len, batch_size, len(abc) + 3).to(device)\n",
    "\n",
    "        conv, hidden = enc(src)\n",
    "        input = torch.zeros(batch_size, dtype = int).to(device)\n",
    "        \n",
    "        for t in range(1, max_len):\n",
    "            output, hidden, attn_weights = dec(input, hidden, conv)\n",
    "            outputs[t] = output\n",
    "            input = output.max(1)[1]\n",
    "\n",
    "        idx = outputs.sum(1).max(1)[1].cpu().data.numpy()\n",
    "        num_text = get_num_from_arr(idx)\n",
    "\n",
    "        results_to_sort.append((x_min, num_text))\n",
    "\n",
    "    results = sorted(results_to_sort, key=lambda x: x[0])\n",
    "    num_list = [x[1] for x in results]\n",
    "\n",
    "    plates_string = ' '.join(num_list)\n",
    "    file_name = img_path[img_path.find('test/'):]\n",
    "\n",
    "    file_name_result.append(file_name)\n",
    "    plates_string_result.append(plates_string)\n",
    "    \n",
    "df_submit = pd.DataFrame({'file_name': file_name_result, 'plates_string': plates_string_result})\n",
    "df_submit.to_csv(os.path.join(DATA_PATH1, 'subm_resnet34_attention.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submit.head(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
